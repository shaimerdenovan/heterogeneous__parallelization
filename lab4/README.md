## Описание работы

Лабораторная работа №4 посвящена изучению архитектуры GPU и оптимизации CUDA-программ. В работе рассматривается влияние различных типов памяти CUDA (global, shared и local) на производительность параллельных алгоритмов. Основное внимание уделено редукции суммы элементов массива и сортировке данных на GPU.

Все задания были выполнены для массивов размером 10 000, 100 000 и 1 000 000 элементов. Измерения времени позволили сравнить эффективность разных подходов к работе с памятью.

## Среда выполнения

Работа выполнялась в среде Google Colab с включённым GPU, так как на моем ноутбуке отсутствует видеокарта NVIDIA и нет возможности запускать CUDA-программы локально. Для измерения времени выполнения CUDA-ядер использовались события cudaEvent, что позволяет учитывать только время работы на GPU без учёта копирования данных между CPU и GPU. Для повышения точности измерений ядра запускались несколько раз, а полученное время усреднялось.

## Задача 1. Генерация данных

В первой части работы был реализован алгоритм генерации массива случайных чисел на CPU. Сгенерированный массив затем копировался в глобальную память GPU для дальнейших вычислений.

## Задача 2. Редукция суммы элементов массива

Во второй задаче была реализована редукция суммы элементов массива в двух вариантах:

Редукция с использованием только глобальной памяти — каждый поток считает частичную сумму и использует atomicAdd для добавления результата в глобальную переменную.

Редукция с использованием global и shared памяти — данные сначала загружаются в разделяемую память блока, после чего выполняется блочная редукция.

Результаты показали, что вариант с использованием shared памяти работает быстрее, так как уменьшается количество обращений к глобальной памяти и атомарных операций.

## Задача 3. Сортировка массива на GPU

В третьей задаче была реализована сортировка массива на GPU. Алгоритм включает следующие этапы:

разбиение массива на тайлы;

пузырьковая сортировка небольших подмассивов в локальной памяти потоков;

слияние отсортированных подмассивов с использованием разделяемой памяти.

Такой подход позволяет снизить количество обращений к глобальной памяти и повысить производительность сортировки.

## Задача 4. Измерение производительности

Для всех реализованных алгоритмов было измерено время выполнения для массивов разных размеров. Результаты были сохранены в CSV-файл и использованы для построения графиков зависимости времени выполнения от размера массива.

## Вывод

В ходе выполнения работы было показано, что производительность CUDA-программ сильно зависит от типа используемой памяти. Использование разделяемой памяти позволяет значительно ускорить вычисления, если данные используются несколько раз. Глобальная память является узким местом и должна использоваться как можно реже. Грамотная организация доступа к памяти позволяет получить заметный прирост производительности без изменения алгоритма.

## Ответы на контрольные вопросы
1. Чем отличаются типы памяти в CUDA и в каких случаях их использовать?

В CUDA есть глобальная, разделяемая и локальная память.
Глобальная память большая, но медленная, и используется для хранения основных массивов данных.
Разделяемая память быстрая, но доступна только внутри одного блока и используется для обмена данными между потоками.
Локальная память принадлежит одному потоку и применяется для временных переменных.

2. Как использование разделяемой памяти влияет на производительность?

Разделяемая память ускоряет выполнение программ, так как она значительно быстрее глобальной. Если данные используются несколько раз разными потоками одного блока, их выгодно хранить в shared памяти. Однако из-за синхронизации потоков shared память не всегда даёт ускорение.

3. Доступ и как его обеспечить?

Для высокой производительности важно обеспечивать коалесцированный доступ к глобальной памяти, когда соседние потоки обращаются к соседним элементам массива. Также необходимо правильно использовать синхронизацию при работе с разделяемой памятью.

4. Какие сложности возникают при работе с большим объёмом данных на GPU?

Основные сложности — это ограниченный объём памяти GPU, медленный доступ к глобальной памяти и необходимость разбивать данные на части. Также увеличивается время копирования данных между CPU и GPU.

5. Почему важно минимизировать доступ к глобальной памяти?

Глобальная память имеет большую задержку доступа. Частые обращения к ней сильно замедляют выполнение программы. Использование shared и local памяти позволяет уменьшить число обращений к global memory и повысить скорость работы.

6. Как использовать профилирование для анализа производительности CUDA-программ?

Для анализа производительности можно использовать cudaEvent для измерения времени работы ядер, а также инструменты NVIDIA, такие как Nsight Systems и Nsight Compute. Профилирование помогает найти узкие места программы и понять, какие участки требуют оптимизации.
